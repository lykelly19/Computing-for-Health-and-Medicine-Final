{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e88a746",
   "metadata": {},
   "source": [
    "# Diabetic Retinopathy Detection: Identify Severity of Diabetic Retinopathy in Eye Images\n",
    "\n",
    "University of Massachusetts Lowell   \n",
    "COMP.5300/COMP.4600 Computing in Health and Medicine  \n",
    "Dr. Wenjin Zhou  \n",
    "Brent Garey, Kelly Ly, Ann Men, Bishoy Sargius, and William Zouzas  \n",
    "Due April 26, 2022  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d07af-11da-4e09-a1be-4b41c1450636",
   "metadata": {},
   "source": [
    "<h1>Git Commands</h1>\n",
    "<h5>Adds changes of the code to our github repo</h5>\n",
    "git add .<br>\n",
    "git commit -m \"some message\"<br>\n",
    "git push<br>\n",
    "\n",
    "<h5>Prints out what happened with the code worked on and github</h5>\n",
    "git status<br>\n",
    "\n",
    "<h5>Downloads the files from the github repo</h5>\n",
    "git pull<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f521a",
   "metadata": {},
   "source": [
    "# STEP 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808cb193",
   "metadata": {},
   "source": [
    "## Read in Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e237e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63ede9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15_left</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35121</th>\n",
       "      <td>44347_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35122</th>\n",
       "      <td>44348_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35123</th>\n",
       "      <td>44348_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35124</th>\n",
       "      <td>44349_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35125</th>\n",
       "      <td>44349_right</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35126 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             image  level\n",
       "0          10_left      0\n",
       "1         10_right      0\n",
       "2          13_left      0\n",
       "3         13_right      0\n",
       "4          15_left      1\n",
       "...            ...    ...\n",
       "35121  44347_right      0\n",
       "35122   44348_left      0\n",
       "35123  44348_right      0\n",
       "35124   44349_left      0\n",
       "35125  44349_right      1\n",
       "\n",
       "[35126 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read csv file\n",
    "df_labels = pd.read_csv(\"data/trainLabels.csv\")\n",
    "\n",
    "#Display metadata\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f8a3f3",
   "metadata": {},
   "source": [
    "## Visualize Level Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5cf7eeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of Diabetic Retinopathy Levels (0-4)')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeaklEQVR4nO3debhcVZ3u8e9LgoAmgJiA4SQSlDw2gxIvMeY2tqKxJQ4Y9AENLRI1GkVQsHEAR6RNo61Iqy20IJhBpgjaoBdULqMDJh4QhRDRXMDkkJABAgQVJOF3/1iryE6lzkmdrFNVOeT9PE89Z9fae61ae9fw7r32PlWKCMzMzLbWDp3ugJmZDW4OEjMzK+IgMTOzIg4SMzMr4iAxM7MiDhIzMyviIGkzSf8t6bMD1NYLJD0maUi+f6Ok9w1E27m9ayRNH6j2+vG4X5S0RtIDBW08JumFTSw3VlJIGrq1j1XX3jsl/Wwg2irsR0eeuy2RNFvSFzvdj61R8v6S9EtJLxvoPuW2F0o6sBVtN8tBMoAk3Sfpb5LWSXpY0q8kfVDS09s5Ij4YEf/WZFuv62uZiFgaEcMiYsMA9P10Sd+ra/8NETGntO1+9mMMcApwQEQ8v8H8wyQ9lYPiMUk9kuZLenl1ubxd7mlxXzcLoYi4KCJevxVtzZb097xOD0m6VtI/NFl3m3ju6kl6t6RftLD9kLRfq9ofKJKOANZFxG8rZR+V9ICkRyRdKGmnJtu6vsGOz1eBMwa42/3iIBl4R0TEcGAf4EvAJ4ELBvpBBmoPehu0D/BgRKzqY5nlETEMGA5MAv4A/FzS5HZ0sIX+I69XF3A/LXjdWEd8EJhXuyPpcOBUYDIwFngh8IUtNSLpnUCj9/1VwGskjRqIzm6ViPBtgG7AfcDr6somAk8BB+X7s4Ev5ukRwI+Bh4GHgJ+Twn1ervM34DHgE6QXXAAzgKXAzZWyobm9G4EzgYXAI8CVwB553mFAT6P+AlOAvwNP5sf7XaW99+XpHYDPAH8GVgFzgd3yvFo/pue+rQE+3cd22i3XX53b+0xu/3V5nZ/K/ZjdoO5m65HL/wvortwPYL88/Sbgt8CjwDLg9Mpytb7PBJYDK4BTKvN3IL3p/x/wIDC/sk2X5rqP5dv/Bt4N/KJS/0Dg2vz8rgQ+1cs2efp1ke+/EfhL5f7ewBV5m90LfCSXN/PcvRv4BWnPdW2u/4a6tq/KfVwCvL8y73TgcuAyYB1wG3BwZX5t26wD7gLemsv3Bx4HNuR+PVxZz28B/yfXWQC8KM/7FnBW3Xb5EXByL9vs6ee4rnynvK5L8zb/b2CXPG8x8ObKskNJr9f/le9PAn5Fek/+Djissmx1m+4H3ER6n60BLuulj88ivaZHV8ouBv69cn8y8MAWPlt2A/6Y+/f0e74y/1pgeic+9yLCQTKgG7NBkOTypcDxeXo2G4PkzPwi3zHf/glQo7bY+IE3F3gOsAuNg+R+4KC8zBXA9/K8w+glSPL06bVlK/Orb5z3kj5kXggMA34AzKvr2/m5XwcDTwD797Kd5pJCbniu+0dgRm/9rKvbcD7wWlIAPSffrwbJYcBLSKHwUtKHy5F1fb8kb7OXkD6sa9vlZODXwGjSB9S3gUvq6g6t9OPd5CDJ67eCNFS3c77/il7WazYbXxfPIe1M1EJhB+BW4HOkD6YXAvcAhzf53L2bFDTvB4YAx5NCs/Zauwk4J/dxfF7/yZW2nwSOIr1GP0YKoh3z/KNJQbQD8A7gL8Co+m1Rt54PkXawhgIXAZfmeRNzv3bI90cAfwX26mWb9RYk/0kKxj3yNv8RcGae9zngosqybwL+kKe7SDsLb8zr88/5/sgG2/QS4NN5uZ2BV/bSxwOp7BDkst8B76jcH5HX5Xl9vO6/BXyUBq+5PP8bwNc68bkXER7aapPlpBd1vSeBUcA+EfFkRPw88quiD6dHxF8i4m+9zJ8XEXdGxF+AzwJvr52ML/RO0gv1noh4DDgNmFY3xPaFiPhbRPyO9GY5uL6R3Jd3AKdFxLqIuA84C3hXYf+WAwJ2r58RETdGxB0R8VRE/J70IfDqusW+kLfrHcB3gWNy+QdIR1c9EfEE6YP1qCaHFt9M2tM8KyIez+u7oI/lPybpYdKe+ivZuE1eTvowOyMi/h7p3M/5wLQm+lDz54g4P9L5tDmk191e+ZzUK4FP5j7eDnyHTZ+PWyPi8oh4Evga6YNzEkBEfD8iludtexnwJ1Ig9OUHEbEwItaTgmR8bqt2JF0bopwG3BgRK5tdSUkiBeZHI+KhiFgH/Dsbt9XFwFskPTvf/5dcBnAscHVEXJ3X51qgmxQs9Z4kDcPunbdbb+eCdic9n1XD8nrW1KaH97JOE4BDgW/28hjkx9i9j/kt5SBpjy7SXli9r5D28n8m6R5JpzbR1rJ+zP8zaS9yRFO97Nveub1q20OBvSpl1aus/kp6w9QbQdqrrm+rq7B/XaQ9tYfrZ0h6haQbJK2W9AhpzLp+m9Rvt73z9D7AD/PFEw+ThkY2sOl692YMadinWV+NiN1Je51/A15c6cPetT7kfnyqyT7UPP3cRMRf8+Qw0nrWPnBr6p+PZZW6TwE9uR6SjpN0e6VfB7Hl11tfr5M5pA908t959M9I4NnArZU+/SSXExFLSM/hETlM3sLGINkHOLpuO7+SFLr1PkHacVkoaZGk9/bSn7VsHhCPAbtW7tem1+Wr/moXklyTL9Q5BzgpB29vhtPgtd8uDpIWy1cTdZHGqDeR91BPiYgXAkcA/1o5YdzbkcmWjljGVKZfQNpzWkMacqjthdWODEb2o93lpDdate31pGGi/ljDxr25alv397Odem8FbstHYvUuJg11jImI3UjDiapbpn67Lc/Ty0jnE3av3HaOiPvZ8jZbBryovysSEUuBk4CvS9olt3NvXR+GR0RtT3lL/ejLcmAPSdUPu/rn4+ltkz/YRgPLJe1DOjI6kTQssztwJxu37db063vAVEkHk86z/E8/668hhfCBlW21W6SLGGouIR1xTgXuyuECaTvPq9vOz4mIL9U/SEQ8EBHvj4i9SUet5/RyBdmfSAdK1WBexKZH6wcDKyPiwUhX/Q3LtzeQQmYCcFm+HP43uU6PpH+qtLE/aRSgIxwkLSJpV0lvBi4ljV/f0WCZN0vaLx+OP0ra061dyruSNBbeX8dKOiDvbZ0BXJ6HM/4I7CzpTZJ2JJ3grl5yuBIYW71Uuc4lwEcl7StpGGm44LIt7CVtJvdlPjBL0vD8YfSvpA+QflHSJenzwPtIe+mNDCftdT8uaSJpOKPeZyU9W+l6/PeQTi5DCp1ZuZ9IGilpap63mnReprfn6cfA8yWdLGmnvL6vaGbd8rDKctJFAAuBRyV9UtIukoZIOqhyyfOWnru+HmcZ6eTymZJ2lvRS0gUdF1UWO0TS2/Jw3smk81+/Jp3LCdJ2QNJ7SEckNSuB0ZKe1Y/+9JA+LOcBV/QxhFvzrNzvnSXtTAqx84GzJe2Z+9WVr5SquRR4Pelc0cWV8u+RjlQOz9t4Z6XLzUfXP6ikoyvla0nbYbPL8PNw4P9l06HUucCM/D59Lum9OLuX9XuEdPQ3Pt9qOw+HkC5UIF86fAjphHtHOEgG3o8krSPt3XyaNKb8nl6WHUd6kT0G3AKcExE35nlnAp/Jh9gf68fjzyO9KB8gjWV/BCAiHgE+RBr/vp90hNJTqff9/PdBSbc1aPfC3PbNpJOtjwMf7ke/qj6cH/8e0pHaxbn9Zu0tqXal1G9IJ8gPi4je/hHwQ8AZ+Xn5HCnI6t1EGma8jjTEVGvr66SjmZ/l+r8GXgFPDxHNAn6Zn6dJ1QbzcNE/k442HyDtnb6mH+v5FdIQytDcxnjStl9Deh53y8tt6bnbkmNIw2nLgR8Cn89BVnMl6bzWWtK5k7flc3p3kc5v3UIKjZcAv6zUu5609/2ApDX96M+c3FYzw1qLSEcgtdt7SJfcLwF+LelR0nusNkxIRKzIff5HNu4w1EJ1KmmHZDXpPfxxGn9OvhxYkF+HV5GGnu7tpY/fpnLOKSJ+AvwHcANpGPHPwOcbVYzkgdot9wvSEczf8/RbSOeSljdqox1qV22YmW1G0umkK6OO3dKyA/iYryIdHYzN52QGPaV/zPxwVP4pcQDbXkC66vHOgW67Wc/Uf2ozs0EoD7ueBHznmRIiABHxyha23dRwaSt5aMvMtgmS9iddeTSK9L8gNkh4aMvMzIr4iMTMzIpsd+dIRowYEWPHju10N8zMBpVbb711TUSMbDRvuwuSsWPH0t3d3elumJkNKpL+3Ns8D22ZmVkRB4mZmRVxkJiZWREHiZmZFXGQmJlZEQeJmZkVcZCYmVkRB4mZmRVxkJiZWZHt7j/b+3LIx+d2ugsD7tavHNfpLpjZM5yPSMzMrIiDxMzMirQsSCSNkXSDpMWSFkk6KZefLul+Sbfn2xsrdU6TtETS3ZIOr5QfIumOPO8bkpTLd5J0WS5fIGlsq9bHzMwaa+URyXrglIjYH5gEnCDpgDzv7IgYn29XA+R504ADgSnAOZKG5OXPBWYC4/JtSi6fAayNiP2As4Evt3B9zMysgZYFSUSsiIjb8vQ6YDHQ1UeVqcClEfFERNwLLAEmShoF7BoRt0T6Oce5wJGVOnPy9OXA5NrRipmZtUdbzpHkIaeXAQty0YmSfi/pQknPzWVdwLJKtZ5c1pWn68s3qRMR64FHgOc1ePyZkrolda9evXpgVsrMzIA2BImkYcAVwMkR8ShpmOpFwHhgBXBWbdEG1aOP8r7qbFoQcV5ETIiICSNHNvyBLzMz20otDRJJO5JC5KKI+AFARKyMiA0R8RRwPjAxL94DjKlUHw0sz+WjG5RvUkfSUGA34KHWrI2ZmTXSyqu2BFwALI6Ir1XKR1UWeytwZ56+CpiWr8Tal3RSfWFErADWSZqU2zwOuLJSZ3qePgq4Pp9HMTOzNmnlf7YfCrwLuEPS7bnsU8AxksaThqDuAz4AEBGLJM0H7iJd8XVCRGzI9Y4HZgO7ANfkG6SgmidpCelIZFoL18fMzBpoWZBExC9ofA7j6j7qzAJmNSjvBg5qUP44cHRBN83MrJD/s93MzIo4SMzMrIiDxMzMijhIzMysiIPEzMyKOEjMzKyIg8TMzIo4SMzMrIiDxMzMijhIzMysiIPEzMyKOEjMzKyIg8TMzIo4SMzMrIiDxMzMijhIzMysiIPEzMyKOEjMzKyIg8TMzIo4SMzMrIiDxMzMijhIzMysiIPEzMyKOEjMzKyIg8TMzIo4SMzMrIiDxMzMijhIzMysiIPEzMyKOEjMzKyIg8TMzIq0LEgkjZF0g6TFkhZJOimX7yHpWkl/yn+fW6lzmqQlku6WdHil/BBJd+R535CkXL6TpMty+QJJY1u1PmZm1lgrj0jWA6dExP7AJOAESQcApwLXRcQ44Lp8nzxvGnAgMAU4R9KQ3Na5wExgXL5NyeUzgLURsR9wNvDlFq6PmZk10LIgiYgVEXFbnl4HLAa6gKnAnLzYHODIPD0VuDQinoiIe4ElwERJo4BdI+KWiAhgbl2dWluXA5NrRytmZtYebTlHkoecXgYsAPaKiBWQwgbYMy/WBSyrVOvJZV15ur58kzoRsR54BHheg8efKalbUvfq1asHaK3MzAzaECSShgFXACdHxKN9LdqgLPoo76vOpgUR50XEhIiYMHLkyC112czM+qGlQSJpR1KIXBQRP8jFK/NwFfnvqlzeA4ypVB8NLM/loxuUb1JH0lBgN+ChgV8TMzPrTSuv2hJwAbA4Ir5WmXUVMD1PTweurJRPy1di7Us6qb4wD3+tkzQpt3lcXZ1aW0cB1+fzKGZm1iZDW9j2ocC7gDsk3Z7LPgV8CZgvaQawFDgaICIWSZoP3EW64uuEiNiQ6x0PzAZ2Aa7JN0hBNU/SEtKRyLQWro+ZmTXQsiCJiF/Q+BwGwORe6swCZjUo7wYOalD+ODmIzMysM/yf7WZmVsRBYmZmRRwkZmZWxEFiZmZFHCRmZlbEQWJmZkUcJGZmVsRBYmZmRRwkZmZWxEFiZmZFHCRmZlbEQWJmZkUcJGZmVsRBYmZmRRwkZmZWxEFiZmZFHCRmZlbEQWJmZkUcJGZmVsRBYmZmRRwkZmZWxEFiZmZFHCRmZlbEQWJmZkUcJGZmVsRBYmZmRRwkZmZWxEFiZmZFHCRmZlbEQWJmZkUcJGZmVqRlQSLpQkmrJN1ZKTtd0v2Sbs+3N1bmnSZpiaS7JR1eKT9E0h153jckKZfvJOmyXL5A0thWrYuZmfWulUcks4EpDcrPjojx+XY1gKQDgGnAgbnOOZKG5OXPBWYC4/Kt1uYMYG1E7AecDXy5VStiZma9a1mQRMTNwENNLj4VuDQinoiIe4ElwERJo4BdI+KWiAhgLnBkpc6cPH05MLl2tGJmZu3TiXMkJ0r6fR76em4u6wKWVZbpyWVdebq+fJM6EbEeeAR4XqMHlDRTUrek7tWrVw/cmpiZWXNBIum6ZsqacC7wImA8sAI4q9Zcg2Wjj/K+6mxeGHFeREyIiAkjR47sV4fNzKxvQ/uaKWln4NnAiHz0UPvw3hXYu78PFhErK22fD/w43+0BxlQWHQ0sz+WjG5RX6/RIGgrsRvNDaWZmNkC2dETyAeBW4B/y39rtSuBb/X2wfM6j5q1A7Yquq4Bp+UqsfUkn1RdGxApgnaRJ+fzHcfmxa3Wm5+mjgOvzeRQzM2ujPo9IIuLrwNclfTgivtmfhiVdAhxGOprpAT4PHCZpPGkI6j5SUBERiyTNB+4C1gMnRMSG3NTxpCvAdgGuyTeAC4B5kpaQjkSm9ad/ZmY2MPoMkpqI+KakfwTGVutExNw+6hzToPiCPpafBcxqUN4NHNSg/HHg6D47bmZmLddUkEiaRzpJfjtQO1KoXY5rZmbbsaaCBJgAHOBzEGZmVq/Z/yO5E3h+KztiZmaDU7NHJCOAuyQtBJ6oFUbEW1rSKzMzGzSaDZLTW9kJMzMbvJq9auumVnfEzMwGp2av2lrHxq8feRawI/CXiNi1VR0zM7PBodkjkuHV+5KOBCa2okNmZja4bNW3/0bE/wCvHdiumJnZYNTs0NbbKnd3IP1fif+nxMzMmr5q64jK9HrS92RNHfDemJnZoNPsOZL3tLojZmY2ODX7w1ajJf1Q0ipJKyVdIWn0lmuamdkzXbMn279L+v2PvUk/cfujXGZmZtu5ZoNkZER8NyLW59tswL9Za2ZmTQfJGknHShqSb8cCD7ayY2ZmNjg0GyTvBd4OPACsIP20rU/Am5lZ05f//hswPSLWAkjaA/gqKWDMzGw71uwRyUtrIQIQEQ8BL2tNl8zMbDBpNkh2kPTc2p18RNLs0YyZmT2DNRsGZwG/knQ56atR3g7MalmvzMxs0Gj2P9vnSuomfVGjgLdFxF0t7ZmZmQ0KTQ9P5eBweJiZ2Sa26mvkzczMahwkZmZWxEFiZmZFHCRmZlbEQWJmZkUcJGZmVsRBYmZmRRwkZmZWpGVBIunC/NO8d1bK9pB0raQ/5b/V7+86TdISSXdLOrxSfoikO/K8b0hSLt9J0mW5fIGksa1aFzMz610rj0hmA1Pqyk4FrouIccB1+T6SDgCmAQfmOudIGpLrnAvMBMblW63NGcDaiNgPOBv4csvWxMzMetWyIImIm4GH6oqnAnPy9BzgyEr5pRHxRETcCywBJkoaBewaEbdERABz6+rU2rocmFw7WjEzs/Zp9zmSvSJiBUD+u2cu7wKWVZbryWVdebq+fJM6EbEeeAR4Xst6bmZmDW0rJ9sbHUlEH+V91dm8cWmmpG5J3atXr97KLpqZWSPtDpKVebiK/HdVLu8BxlSWGw0sz+WjG5RvUkfSUGA3Nh9KAyAizouICRExYeTIkQO0KmZmBu0PkquA6Xl6OnBlpXxavhJrX9JJ9YV5+GudpEn5/MdxdXVqbR0FXJ/Po5iZWRu17OdyJV0CHAaMkNQDfB74EjBf0gxgKXA0QEQskjSf9Hsn64ETImJDbup40hVguwDX5BvABcA8SUtIRyLTWrUuZmbWu5YFSUQc08usyb0sP4sGP98bEd3AQQ3KHycHkZmZdc62crLdzMwGKQeJmZkVcZCYmVkRB4mZmRVxkJiZWREHiZmZFXGQmJlZEQeJmZkVcZCYmVkRB4mZmRVxkJiZWREHiZmZFXGQmJlZEQeJmZkVcZCYmVkRB4mZmRVxkJiZWREHiZmZFXGQmJlZEQeJmZkVcZCYmVkRB4mZmRVxkJiZWREHiZmZFXGQmJlZEQeJmZkVcZCYmVkRB4mZmRVxkJiZWREHiZmZFXGQmJlZEQeJmZkVGdqJB5V0H7AO2ACsj4gJkvYALgPGAvcBb4+ItXn504AZefmPRMRPc/khwGxgF+Bq4KSIiHauiz3zHfrNQzvdhQH3yw//stNdsGeQTh6RvCYixkfEhHz/VOC6iBgHXJfvI+kAYBpwIDAFOEfSkFznXGAmMC7fprSx/2ZmxrY1tDUVmJOn5wBHVsovjYgnIuJeYAkwUdIoYNeIuCUfhcyt1DEzszbpVJAE8DNJt0qamcv2iogVAPnvnrm8C1hWqduTy7rydH35ZiTNlNQtqXv16tUDuBpmZtaRcyTAoRGxXNKewLWS/tDHsmpQFn2Ub14YcR5wHsCECRN8DsXMbAB15IgkIpbnv6uAHwITgZV5uIr8d1VevAcYU6k+Gliey0c3KDczszZqe5BIeo6k4bVp4PXAncBVwPS82HTgyjx9FTBN0k6S9iWdVF+Yh7/WSZokScBxlTpmZtYmnRja2gv4YfrsZyhwcUT8RNJvgPmSZgBLgaMBImKRpPnAXcB64ISI2JDbOp6Nl/9ek29mZtZGbQ+SiLgHOLhB+YPA5F7qzAJmNSjvBg4a6D6amVnztqXLf83MbBBykJiZWREHiZmZFXGQmJlZEQeJmZkVcZCYmVkRB4mZmRVxkJiZWREHiZmZFXGQmJlZEQeJmZkVcZCYmVkRB4mZmRVxkJiZWZFO/dSubeOWnvGSTndhwL3gc3d0ugtmz0g+IjEzsyIOEjMzK+IgMTOzIg4SMzMr4iAxM7MiDhIzMyviIDEzsyIOEjMzK+IgMTOzIg4SMzMr4iAxM7MiDhIzMyviL200s6bd9KpXd7oLA+7VN9/U6S4Meg4SM7Ot8F+n/KjTXRhwJ551xFbV89CWmZkVcZCYmVmRQR8kkqZIulvSEkmndro/Zmbbm0EdJJKGAN8C3gAcABwj6YDO9srMbPsyqIMEmAgsiYh7IuLvwKXA1A73ycxsu6KI6HQftpqko4ApEfG+fP9dwCsi4sS65WYCM/PdFwN3t7WjjY0A1nS6E9sIb4vE22Ejb4uNtpVtsU9EjGw0Y7Bf/qsGZZslY0ScB5zX+u40T1J3REzodD+2Bd4WibfDRt4WGw2GbTHYh7Z6gDGV+6OB5R3qi5nZdmmwB8lvgHGS9pX0LGAacFWH+2Rmtl0Z1ENbEbFe0onAT4EhwIURsajD3WrWNjXU1mHeFom3w0beFhtt89tiUJ9sNzOzzhvsQ1tmZtZhDhIzMyviIGkzf6VLIulCSask3dnpvnSapDGSbpC0WNIiSSd1uk+dImlnSQsl/S5viy90uk+dJmmIpN9K+nGn+9IbB0kb+StdNjEbmNLpTmwj1gOnRMT+wCTghO34dfEE8NqIOBgYD0yRNKmzXeq4k4DFne5EXxwk7eWvdMki4mbgoU73Y1sQESsi4rY8vY70odHV2V51RiSP5bs75tt2e0WQpNHAm4DvdLovfXGQtFcXsKxyv4ft9APDGpM0FngZsKDDXemYPJRzO7AKuDYittttAfwn8AngqQ73o08OkvZq6itdbPskaRhwBXByRDza6f50SkRsiIjxpG+qmCjpoA53qSMkvRlYFRG3drovW+IgaS9/pYs1JGlHUohcFBE/6HR/tgUR8TBwI9vvubRDgbdIuo80DP5aSd/rbJcac5C0l7/SxTYjScAFwOKI+Fqn+9NJkkZK2j1P7wK8DvhDRzvVIRFxWkSMjoixpM+K6yPi2A53qyEHSRtFxHqg9pUui4H5g+grXQaUpEuAW4AXS+qRNKPTfeqgQ4F3kfY4b8+3N3a6Ux0yCrhB0u9JO17XRsQ2e9mrJf6KFDMzK+IjEjMzK+IgMTOzIg4SMzMr4iAxM7MiDhIzMyviIDFrMUmPbXmpzrdptrUcJGZmVsRBYtZGkj4u6TeSfl/7rQ1JX5b0ocoyp0s6pbflzbY1DhKzNpH0emAc6ecExgOHSHoV6XuU3lFZ9O3A9/tY3mybMrTTHTDbjrw+336b7w8DxkXEBZL2lLQ3MBJYGxFLJX2k0fLAzW3ut1mfHCRm7SPgzIj4doN5lwNHAc8nHaFsaXmzbYaHtsza56fAe/PvjiCpS9Keed6lpG94PYoUKlta3myb4SMSszaJiJ9J2h+4JX1zPI8Bx5J+vGiRpOHA/RGxYkvLd2QFzHrhb/81M7MiHtoyM7MiDhIzMyviIDEzsyIOEjMzK+IgMTOzIg4SMzMr4iAxM7Mi/x+HxU7nt+VZ+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import Seaborn library\n",
    "# Source: https://www.geeksforgeeks.org/countplot-using-seaborn-in-python/\n",
    "import seaborn as sns\n",
    "  \n",
    "# count plot on single categorical variable\n",
    "sns.countplot(x = df_labels['level'])\n",
    " \n",
    "# Show the plot\n",
    "plt.title('Distribution of Diabetic Retinopathy Levels (0-4)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dbf8dc",
   "metadata": {},
   "source": [
    "## Create and Organize Data Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a05a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os library\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe1f485a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/labeled_data already exists\n"
     ]
    }
   ],
   "source": [
    "# Make new folder to hold labeled data\n",
    "try:\n",
    "    os.mkdir('./data/labeled_data')\n",
    "except FileExistsError:\n",
    "    print('./data/labeled_data already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7c55ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/labeled_data/0 already exists\n",
      "./data/labeled_data/1 already exists\n",
      "./data/labeled_data/2 already exists\n",
      "./data/labeled_data/3 already exists\n",
      "./data/labeled_data/4 already exists\n"
     ]
    }
   ],
   "source": [
    "# Create subfolders for each level\n",
    "for i in range(5):\n",
    "    try:\n",
    "        os.mkdir('./data/labeled_data/' + str(i))\n",
    "    except FileExistsError:\n",
    "        print('./data/labeled_data/' + str(i) + ' already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1a7464",
   "metadata": {},
   "source": [
    "## Visualize Sample Eye Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a76fdc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the data type of the levels? (0-4) Strings or integers?\n",
    "type(df_labels['level'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bcb87-5022-41a1-bb11-ecc53b1797d9",
   "metadata": {},
   "source": [
    "<h1>Creating Image Subset from labeled data file</h1>\n",
    "The dataset from kaggle is about 88 GB of images which is huge to download on our local machine so our group decided to just simply use a subset of the kaggle dataset. On my local machine I extracted just one of the zip files from Kaggle using 7-zip. A single zip file is about 8 GB which is still very large so I would make a subset of the single zip file that I had extracted that would hold about 1000 images. I am choosing 1000 images because the Inception v2 research article that the group had read had a dataset that was 1200 images.<br><br>\n",
    "The goal of this code is to just grab 1000 images from the training zip file I created on my local machine. I will then upload the smaller dataset to our github repository if the file size is small enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04dfc4",
   "metadata": {},
   "source": [
    "The code cell below is commented out since we have already took a subset of the images and don't need to run this cell again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28b5fb71-a2e2-4e3c-8059-83c8f1b873a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This is the absolute file path on my machine\n",
    "# bishoy_local_kaggle_data_dir = r\"C:\\Users\\bisho\\Desktop\\College\\Classes\\Junior\\Spring 2022\\CompHealthMedicine\\Final Project\\data\\train\"\n",
    "\n",
    "# image_names_list = sorted(os.listdir(bishoy_local_kaggle_data_dir))[:1000]\n",
    "# print(\"Length of image_names_list is: \", len(image_names_list), '\\n', image_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24ef2cca-9269-45d0-9b78-9c64abd0739d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "data_dir = \"./data/labeled_data/\"  # This is the data folder in the git repo that holds that label folders from 0 to 4\n",
    "image_value_dic = {}  # This is a dictionary that will take the image name and the associated value from the df_labels dataframe\n",
    "\n",
    "for i in range(len(df_labels)):\n",
    "    # Grabs the image name from \"image\" column in df_labels and sets that as key then grabs value from \"value\" column and sets that as value\n",
    "    image_value_dic[str(df_labels.iloc[i]['image']) + \".jpeg\"] = df_labels.iloc[i]['level']\n",
    "    \n",
    "# print(image_value_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afd5c8d",
   "metadata": {},
   "source": [
    "The code cell below is commented out since we have already sorted & pushed the images into their appropriate subfolders and don't need to run this cell again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "151b4123-b0f8-4237-bb82-247966e659d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # this for loop sorts the 1000 image names in image_names_list into the appropiate dr severity folder from 0 to 4\n",
    "# for image in image_names_list:\n",
    "#     origin_file = bishoy_local_kaggle_data_dir + \"\\\\\" + image  # This is the file path of the current image we are looking at\n",
    "#     # Find image level from image_value_dic\n",
    "#     place_into_dir = os.path.join(data_dir, str(image_value_dic[image]) + \"/\" + image) # Depending on machine's OS may have to switch \"/\" with \"\\\" or vice versa\n",
    "#     # copy image from abs file path to labeled_data file that is in the repo\n",
    "#     shutil.copy(origin_file, place_into_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70091b-b28b-4fe1-b5d3-b521671239bc",
   "metadata": {},
   "source": [
    "<h1>Dealing with Class Imbalance</h1>\n",
    "I want to see how many images are in each folder and figure out what the weights of those images should be. This is similar to what we did in hw5/hw6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "964a8848-fc3c-4de9-bd73-48a96453db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:  728 \n",
      " 1:  66 \n",
      " 2:  159 \n",
      " 3:  26 \n",
      " 4:  21 \n",
      " total:  1000\n"
     ]
    }
   ],
   "source": [
    "num_of_0_imgs = len(os.listdir('data/labeled_data/0'))  # Goes into the specified severity level folder and counts number of images inside\n",
    "num_of_1_imgs = len(os.listdir('data/labeled_data/1'))\n",
    "num_of_2_imgs = len(os.listdir('data/labeled_data/2'))\n",
    "num_of_3_imgs = len(os.listdir('data/labeled_data/3'))\n",
    "num_of_4_imgs = len(os.listdir('data/labeled_data/4'))\n",
    "\n",
    "img_total = (num_of_0_imgs + num_of_1_imgs + \n",
    "             num_of_2_imgs + num_of_3_imgs + \n",
    "             num_of_4_imgs)  # adds up all of the images in the folders and sets it to a variable\n",
    "\n",
    "print(\"0: \", num_of_0_imgs, \"\\n\",\n",
    "      \"1: \", num_of_1_imgs, \"\\n\",\n",
    "      \"2: \", num_of_2_imgs, \"\\n\",\n",
    "      \"3: \", num_of_3_imgs, \"\\n\",\n",
    "      \"4: \", num_of_4_imgs, \"\\n\",\n",
    "      \"total: \", img_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe68e1d",
   "metadata": {},
   "source": [
    "Now we want to visualize distribution of classes in our subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3480ab32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of Diabetic Retinopathy Levels (0-4) Subset')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdaElEQVR4nO3de5xdZX3v8c+XBBIE5FKGNCSRQM2LElCixogHbdGoREFCW6mx6ok2SmnRgsWjiXc8pnDaavWcklZESuQW461EbK05UbxLDBiUEJCUQDJMSCZghIAGEn7943k2WdnZe2bPzJ7s5Mn3/XrNa6/1rMv+rbXX/u61n7VmRhGBmZmV5YBOF2BmZu3ncDczK5DD3cysQA53M7MCOdzNzArkcDczK9A+Fe6S/kXSh9u0rudI2ippRB6/RdI72rHuvL7/kDS7XesbwPN+QtJmSQ8NYR1bJZ3QwnwTJYWkkYN9rrr1vVnSt9qxriHW0ZHXrj+SrpH0iU7XMRhDeX9J+qGkF7S7przu5ZJOHqZ1nyGpezjW3Yq9Jtwl3S/pN5Iek7RF0o8kXSDpmRoj4oKI+N8trutVfc0TEesi4tCI2NGG2j8m6bq69b82IhYOdd0DrGMCcAkwOSJ+t8H0MyQ9ncN7q6RuSYslvbg6X94v9w1zrbt9METE9RHxmkGs6xpJT+ZtekTSUkm/3+Kye8VrV0/S2yT9YBjXH5KeO1zrbxdJrwcei4ifVdreI+khSb+WdLWkUS2u69sNTkb+Afh4H8scJOmT+b2yVdJaSf846A1qg1byDfaicM9eHxGHAccBlwPvBz7f7idp15nmXug44OGI2NTHPD0RcShwGHAacDfwfUnT90SBw+jv8naNAx5kGI4b64gLgGtrI5LOBOYC04GJwAnApf2tRNKbgUbv+yXAKySNbbLoPGAqMI30nnkF8LMm8+5dImKv+AHuB15V1zYNeBo4JY9fA3wiDx8N3AxsAR4Bvk/6sLo2L/MbYCvwPtJBEMAcYB3wvUrbyLy+W4DLgOXAr4GbgKPytDOA7kb1AjOAJ4Gn8vPdUVnfO/LwAcCHgAeATcAXgMPztFods3Ntm4EP9rGfDs/L9+b1fSiv/1V5m5/OdVzTYNndtiO3/xOwojIewHPz8Fmkg/lRYD3wscp8tdrPB3qADcAllekHkN6I/wU8DCyu7NN1edmt+eelwNuAH1SWPxlYml/fjcAHmuyTZ46LPP464PHK+LHAV/I+Wwv8dW5v5bV7G/AD0hner/Lyr61b95Jc4xrgnZVpHwO+DHwReAy4HTi1Mr22bx4D7gL+KLefBPwW2JHr2lLZziuAb+RlbgV+L0+7Avhk3X75OnBxk332zGtc1z4qb+u6vM//BTg4T1sNnF2ZdyTpeH1hHj8N+BHpPXkHcEZl3uo+fS7wXdL7bDPwxSY1HkQ6psdX2m4A/rYyPh14qJ9sORz4Za7vmfd8ZfpSYHaTZW9utg8b7Ud2zagzgG7gA3k77wfeXHec3pVfyweB91amnQ2szPvyR8Dzc/tu+da0tr52yp78oUG4V0LgLxvsuMvygXdg/nk5oEbrYmcIfQE4BDiYxuH+IHBKnucrwHXVF6lZvaQ38XV106sH85+T3vgnAIcCXwWuravtc7muU4FtwElN9tMXSB88h+VlfwnMaVZn3bINpwOvzAfMIfUHbF7meaSgfj7pDX9uXe035n32PFKA1vbLxcBPgPGk0PgscGPdsiMrdbyNHO55+zaQuplG5/GXNNmua9h5XBxCegPUgvoA4DbgI6SwOAG4DzizxdfubaTwfycwAvhL0gdZ7Vj7LrAg1zglb//0yrqfAt5AOkbfS/pwODBPP4/04XAA8EbgcWBs/b6o285HSCc9I4HrgUV52rRc1wF5/GjgCWBMK6FUaf806cPqqLzPvw5clqd9BLi+Mu9ZwN15eBzpA/x1eXtence7GuzTG4EP5vlGAy9rUuPJVD6kc9sdwBsr40fnbfmdPo77K4D30OCYy9P/L/CpJst+iJRBf0U6vtXXfmT3cN8OfIp0/P9hfo1PzNM3AC/Pw0ey80PyhaSTwJeQjrnZpLwZ1VdW1v/sbd0yjfSQDrR6TwFjgeMi4qmI+H7kLe/DxyLi8Yj4TZPp10bEnRHxOPBh4E9rF1yH6M2kg+e+iNhK+qo3q6576NKI+E1E3EE6gE+tX0mu5Y3AvIh4LCLuBz4JvHWI9fUAAo6onxARt0TELyLi6Yj4OemN+Yd1s12a9+svgH8F3pTb/4L0LaQ7IraRwu4NLXaLnU06I/tkRPw2b++tfcz/XklbSGdBL2PnPnkxKWA+HhFPRrqW8DlgVgs11DwQEZ+LdH1mIem4G5OvcbwMeH+ucSVwFbu+HrdFxJcj4inSm3w06QySiPhSRPTkfftF4F5SSPflqxGxPCK2k8J9Sl5X7RtnrXttFnBLRGxsdSMlifQh9p6IeCQiHgP+lp376gbgHEnPyuN/ltsA3gL8e0T8e96epcAKUtjXe4rUhXhs3m/Nri0cQXo9qw7N21lTGz6syTZNBU4H/l+T5yA/xxFNpl0G/B/Se3gF8OAgLrZ/OCK2RcR3Sd+6/jS3PwVMlvTsiPhVRNye298JfDYibo2IHZGu/2wjHzet2hfCfRzpbKXe35POhr8l6T5Jc1tY1/oBTH+AdLZ1dEtV9u3YvL7qukcCYypt1btbniAdxPWOJp191q9r3BDrG0c6A9lSP0HSSyR9R1KvpF+T+kDr90n9fjs2Dx8HfC1fIN9C+lq/g123u5kJpC6LVv1DRBxBOjv7DXBipYZjazXkOj7QYg01z7w2EfFEHjyUtJ21EKypfz3WV5Z9mvQ1/VgASf9T0spKXafQ//HW13GykBSy5MdrGZgu4FnAbZWavpnbiYg1pNfw9Tngz2FnuB8HnFe3n19G+iCs9z7SycRySask/XmTen7F7qG9FXh2Zbw2/Fi+26p2s8B/5JsxFgAX5Q/DZg6jwbEPkMP1iog4nfQBMB+4WtJJfaxvl23IJ4s11ffHn5A+/B6Q9F1JL83txwGX1O3LCZXlWrJXh3u+i2Mcqc9zF/lM7pKIOAF4PfA3lYuCzc7g+zuzn1AZfg7pk3Uz6atU7WyldgbdNYD19pBesOq6t5O6OAZiMzvPeqrrenCA66n3R8DtdQdhzQ2kr+kTIuJwUleY6uap3289eXg9qX/6iMrP6Ih4kP732Xrg9wa6IRGxDrgI+Iykg/N61tbVcFhE1M4o+6ujLz3AUZKqAVT/ejyzb3LYjAd6JB1H+gbxLlKXwhHAnezct4Op6zpgpqRTSf32/zbA5TeTPhhPruyrwyNdqK65kfTNbCZwVw58SPv52rr9fEhEXF7/JBHxUES8MyKOJX27W9Dkzp17SV8oqh+Wq9j1W+2pwMaIeDjS3VaH5p/XkoJ/KvBFpVuDf5qX6Zb08so6TiJ9W+5T/mZ9BelDZ3JufoJKNgD1d6kdKemQyvgz74+I+GlEzASOIb1Wi/M864H5dfvyWRFxY62U/mqFvTTcJT1b0tnAIlJ/6C8azHO2pOfmr5KPks4Ia7c1biT1rQ7UWyRNzmclHwe+nL+K/xIYLeksSQeS+uGqt19tBCZWb9uscyPwHknHSzqU9FX3i/2cTewm17IYmC/psBwQf0N6Uw+IknGSPgq8g3Q228hhpLPT30qaRvoqXu/Dkp6ldL/w20kXECF9EMzPdSKpS9LMPK2X1M/f7HW6GfhdSRdLGpW39yWtbFvuEughXehdDjwq6f2SDpY0QtIp2nn7Z3+vXV/Ps550sesySaMlPZ900f76ymwvkvTHuSvqYtLX65+Qrg0EaT8g6e2kM/eajcB4SQcNoJ5uUoBdC3ylj+7HmoNy3aMljSZ9sHwO+EdJx+S6xuU7VGoWAa8hXXu4odJ+HemM/sy8j0cr3Xo7vv5JJZ1Xaf8VaT/sdkty7sr6/+zaDfgFYE5+nx5Jei9e02T7fk06252Sf2of6C8iXYxG6TbKF5Euqu4mH39n5GNnZO6SOYydd8ysBP4sb/MMdu+yBLhU6ZbKl5O6G7+Ux98s6fC8nbUMg/QaXJC/NUvSITl7aicRLeXb3hbuX5f0GOmT64OkPsq3N5l3EumF3wr8GFgQEbfkaZcBH8pfad47gOe/lnSgPETqG/1rgIj4NemCylWks7LHSV+va76UHx+WdDu7uzqv+3ukC2q/Bd49gLqq3p2f/z7SN5ob8vpbdayk2h0qPyVdJDojIpr98tBfAR/Pr8tH2Hl2UfVdUhfZMlL3SG1dnyGd9X8rL/8T0kWiWvfGfOCH+XXapT8xd3W8mvSt7CHSWdwrBrCdf0/6+j8yr2MKad9vJr2Oh+f5+nvt+vMmUldQD/A14KP5w6XmJtJ1kl+R+uL/ONI1ortI10t+THqzPg/4YWW5b5POUh+StHkA9SzM62qlS2YV6Uy99vN20u3Ha4CfSHqU9B6rdXERERtyzf+DnR/itQ+6maSThF7Se/h/0ThjXgzcmo/DJaRuk7VNavwslWsYEfFN4O+A75C6OB4APtpowUgeqv3kuiCd6T+Zh88hXZvoabQO0n75JOkY3AxcCPxJ7Pw9kItIx9cWUr/8v9Ut/xDpte8hfehfEBF352lvBe7P+/kCcpdaRKwg9bv/U152DekCe01L+Va74m9mbSbpY6Q7Kd7S37xtfM4/IJ1FT8x9/Ps8pV/mendUfpGpjeu+lXS32Z3tXnenlfrLPGb7ndxleBFwVSnBDhARLxvGdbfU1bcv2tu6ZcxsEPLdG1tId6d8uqPF2F7B3TJmZgXymbuZWYH2ij73o48+OiZOnNjpMszM9im33Xbb5ojoajRtrwj3iRMnsmLFik6XYWa2T5H0QLNp7pYxMyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyvQXvEbqkM1ce43Ol1CW9x/+VmdLsHMCuEzdzOzAjnczcwK5HA3MyuQw93MrED9hrukEyWtrPw8KuliSUdJWirp3vx4ZGWZeZLWSLpH0pnDuwlmZlav33CPiHsiYkpETAFeBDwBfA2YCyyLiEnAsjyOpMnALOBkYAawQNKI4SnfzMwaGWi3zHTgvyLiAWAmsDC3LwTOzcMzgUURsS0i1gJrgGltqNXMzFo00HCfBdyYh8dExAaA/HhMbh8HrK8s053bdiHpfEkrJK3o7e0dYBlmZtaXlsNd0kHAOcCX+pu1QVvs1hBxZURMjYipXV0N/wWgmZkN0kDO3F8L3B4RG/P4RkljAfLjptzeDUyoLDce6BlqoWZm1rqBhPub2NklA7AEmJ2HZwM3VdpnSRol6XhgErB8qIWamVnrWvrbMpKeBbwa+ItK8+XAYklzgHXAeQARsUrSYuAuYDtwYUTsaGvVZmbWp5bCPSKeAH6nru1h0t0zjeafD8wfcnVmZjYo/g1VM7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK5DD3cysQA53M7MCOdzNzArkcDczK1BL4S7pCElflnS3pNWSXirpKElLJd2bH4+szD9P0hpJ90g6c/jKNzOzRlo9c/8M8M2I+H3gVGA1MBdYFhGTgGV5HEmTgVnAycAMYIGkEe0u3MzMmus33CU9G/gD4PMAEfFkRGwBZgIL82wLgXPz8ExgUURsi4i1wBpgWnvLNjOzvrRy5n4C0Av8q6SfSbpK0iHAmIjYAJAfj8nzjwPWV5bvzm27kHS+pBWSVvT29g5pI8zMbFethPtI4IXAP0fEC4DHyV0wTahBW+zWEHFlREyNiKldXV0tFWtmZq1pJdy7ge6IuDWPf5kU9hsljQXIj5sq80+oLD8e6GlPuWZm1op+wz0iHgLWSzoxN00H7gKWALNz22zgpjy8BJglaZSk44FJwPK2Vm1mZn0a2eJ87waul3QQcB/wdtIHw2JJc4B1wHkAEbFK0mLSB8B24MKI2NH2ys3MrKmWwj0iVgJTG0ya3mT++cD8wZdlZmZD4d9QNTMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswI53M3MCuRwNzMrkMPdzKxADnczswK1FO6S7pf0C0krJa3IbUdJWirp3vx4ZGX+eZLWSLpH0pnDVbyZmTU2kDP3V0TElIio/aPsucCyiJgELMvjSJoMzAJOBmYACySNaGPNZmbWj6F0y8wEFubhhcC5lfZFEbEtItYCa4BpQ3geMzMboFbDPYBvSbpN0vm5bUxEbADIj8fk9nHA+sqy3bnNzMz2kJEtznd6RPRIOgZYKunuPuZVg7bYbab0IXE+wHOe85wWyzAzs1a0dOYeET35cRPwNVI3y0ZJYwHy46Y8ezcwobL4eKCnwTqvjIipETG1q6tr8FtgZma76TfcJR0i6bDaMPAa4E5gCTA7zzYbuCkPLwFmSRol6XhgErC83YWbmVlzrXTLjAG+Jqk2/w0R8U1JPwUWS5oDrAPOA4iIVZIWA3cB24ELI2LHsFRvZmYN9RvuEXEfcGqD9oeB6U2WmQ/MH3J1ZmY2KP4NVTOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK5HA3MyuQw93MrEAth7ukEZJ+JunmPH6UpKWS7s2PR1bmnSdpjaR7JJ05HIWbmVlzAzlzvwhYXRmfCyyLiEnAsjyOpMnALOBkYAawQNKI9pRrZmataCncJY0HzgKuqjTPBBbm4YXAuZX2RRGxLSLWAmuAaW2p1szMWtLqmfungfcBT1faxkTEBoD8eExuHwesr8zXndt2Iel8SSskrejt7R1o3WZm1od+w13S2cCmiLitxXWqQVvs1hBxZURMjYipXV1dLa7azMxaMbKFeU4HzpH0OmA08GxJ1wEbJY2NiA2SxgKb8vzdwITK8uOBnnYWbWZmfev3zD0i5kXE+IiYSLpQ+u2IeAuwBJidZ5sN3JSHlwCzJI2SdDwwCVje9srNzKypVs7cm7kcWCxpDrAOOA8gIlZJWgzcBWwHLoyIHUOu1MzMWjagcI+IW4Bb8vDDwPQm880H5g+xNjMzGyT/hqqZWYEc7mZmBXK4m5kVyOFuZlYgh7uZWYEc7mZmBXK4m5kVyOFuZlYgh7uZWYEc7mZmBXK4m5kVyOFuZlYgh7uZWYEc7mZmBXK4m5kVyOFuZlYgh7uZWYEc7mZmBXK4m5kVqN9wlzRa0nJJd0haJenS3H6UpKWS7s2PR1aWmSdpjaR7JJ05nBtgZma7a+XMfRvwyog4FZgCzJB0GjAXWBYRk4BleRxJk4FZwMnADGCBpBHDULuZmTXRb7hHsjWPHph/ApgJLMztC4Fz8/BMYFFEbIuItcAaYFo7izYzs7611OcuaYSklcAmYGlE3AqMiYgNAPnxmDz7OGB9ZfHu3Fa/zvMlrZC0ore3dwibYGZm9VoK94jYERFTgPHANEmn9DG7Gq2iwTqvjIipETG1q6urpWLNzKw1A7pbJiK2ALeQ+tI3ShoLkB835dm6gQmVxcYDPUMt1MzMWtfK3TJdko7IwwcDrwLuBpYAs/Nss4Gb8vASYJakUZKOByYBy9tct5mZ9WFkC/OMBRbmO14OABZHxM2SfgwsljQHWAecBxARqyQtBu4CtgMXRsSO4SnfzMwa6TfcI+LnwAsatD8MTG+yzHxg/pCrMzOzQfFvqJqZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgXqN9wlTZD0HUmrJa2SdFFuP0rSUkn35scjK8vMk7RG0j2SzhzODTAzs921cua+HbgkIk4CTgMulDQZmAssi4hJwLI8Tp42CzgZmAEskDRiOIo3M7PG+g33iNgQEbfn4ceA1cA4YCawMM+2EDg3D88EFkXEtohYC6wBprW5bjMz68OA+twlTQReANwKjImIDZA+AIBj8mzjgPWVxbpzW/26zpe0QtKK3t7eQZRuZmbNtBzukg4FvgJcHBGP9jVrg7bYrSHiyoiYGhFTu7q6Wi3DzMxa0FK4SzqQFOzXR8RXc/NGSWPz9LHAptzeDUyoLD4e6GlPuWZm1opW7pYR8HlgdUR8qjJpCTA7D88Gbqq0z5I0StLxwCRgeftKNjOz/oxsYZ7TgbcCv5C0Mrd9ALgcWCxpDrAOOA8gIlZJWgzcRbrT5sKI2NHuws3MrLl+wz0ifkDjfnSA6U2WmQ/MH0JdZmY2BP4NVTOzAjnczcwK5HA3MyuQw93MrEAOdzOzAjnczcwK1Mp97mZ7pYlzv9HpEtrm/svP6nQJVhifuZuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgVyuJuZFcjhbmZWIIe7mVmBHO5mZgXqN9wlXS1pk6Q7K21HSVoq6d78eGRl2jxJayTdI+nM4SrczMyaa+XM/RpgRl3bXGBZREwCluVxJE0GZgEn52UWSBrRtmrNzKwl/YZ7RHwPeKSueSawMA8vBM6ttC+KiG0RsRZYA0xrT6lmZtaqwfa5j4mIDQD58ZjcPg5YX5mvO7ftRtL5klZIWtHb2zvIMszMrJF2X1BVg7ZoNGNEXBkRUyNialdXV5vLMDPbvw023DdKGguQHzfl9m5gQmW+8UDP4MszM7PBGGy4LwFm5+HZwE2V9lmSRkk6HpgELB9aiWZmNlD9/g9VSTcCZwBHS+oGPgpcDiyWNAdYB5wHEBGrJC0G7gK2AxdGxI5hqt3MzJroN9wj4k1NJk1vMv98YP5QijIzs6Hxb6iamRXI4W5mViCHu5lZgRzuZmYF6veCqu3dJs79RqdLaJv7Lz+r0yWYFcNn7mZmBXK4m5kVyOFuZlYgh7uZWYEc7mZmBXK4m5kVyOFuZlYgh7uZWYEc7mZmBXK4m5kVyH9+wGwfVcqfnvCfnRgePnM3MyuQz9zNbJ9TyrcWGL5vLj5zNzMrkMPdzKxAwxbukmZIukfSGklzh+t5zMxsd8MS7pJGAFcArwUmA2+SNHk4nsvMzHY3XGfu04A1EXFfRDwJLAJmDtNzmZlZHUVE+1cqvQGYERHvyONvBV4SEe+qzHM+cH4ePRG4p+2FtNfRwOZOF9Eh+/O2w/69/fvztsPev/3HRURXownDdSukGrTt8ikSEVcCVw7T87edpBURMbXTdXTC/rztsH9v//687bBvb/9wdct0AxMq4+OBnmF6LjMzqzNc4f5TYJKk4yUdBMwClgzTc5mZWZ1h6ZaJiO2S3gX8JzACuDoiVg3Hc+1B+0wX0jDYn7cd9u/t35+3Hfbh7R+WC6pmZtZZ/g1VM7MCOdzNzArkcO/H/vxnFCRdLWmTpDs7XcueJmmCpO9IWi1plaSLOl3TniRptKTlku7I239pp2va0ySNkPQzSTd3upbBcLj3wX9GgWuAGZ0uokO2A5dExEnAacCF+9lrvw14ZUScCkwBZkg6rbMl7XEXAas7XcRgOdz7tl//GYWI+B7wSKfr6ISI2BARt+fhx0hv8nGdrWrPiWRrHj0w/+w3d19IGg+cBVzV6VoGy+Het3HA+sp4N/vRG9wSSROBFwC3driUPSp3S6wENgFLI2J/2v5PA+8Dnu5wHYPmcO9bv39Gwcom6VDgK8DFEfFop+vZkyJiR0RMIf2G+TRJp3S4pD1C0tnApoi4rdO1DIXDvW/+Mwr7MUkHkoL9+oj4aqfr6ZSI2ALcwv5z/eV04BxJ95O6Yl8p6brOljRwDve++c8o7KckCfg8sDoiPtXpevY0SV2SjsjDBwOvAu7uaFF7SETMi4jxETGR9J7/dkS8pcNlDZjDvQ8RsR2o/RmF1cDiAv6MQssk3Qj8GDhRUrekOZ2uaQ86HXgr6axtZf55XaeL2oPGAt+R9HPSSc7SiNgnbwncX/nPD5iZFchn7mZmBXK4m5kVyOFuZlYgh7uZWYEc7mZmBXK4m5kVyOFuZlag/wZmmc+HSdDxRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_lengths = [num_of_0_imgs, num_of_1_imgs, num_of_2_imgs, num_of_3_imgs, num_of_4_imgs]\n",
    "classes = [str(i) for i in range(5)]\n",
    "\n",
    "# https://www.tutorialspoint.com/matplotlib/matplotlib_bar_plot.htm\n",
    "plt.bar(classes, image_lengths)\n",
    " \n",
    "# Show the plot\n",
    "plt.title('Distribution of Diabetic Retinopathy Levels (0-4) Subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1298f4-ecf8-4e68-8f60-35209b70f11e",
   "metadata": {},
   "source": [
    "Noticed a very large disproportion of the labeled images for levels 1, 3, and 4 so we should try to fix this with over sampling and then median frequency balancing to determine the weights of each image after getting a better destribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e26f33c5-422b-45aa-8228-5fff93793747",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified: 'd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25340/3311245711.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabeled_folder_0_dir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m67\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# replace 67 images from 0 severity to 1 severity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# look up image name in dictionary with value 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'd'"
     ]
    }
   ],
   "source": [
    "# for i = (the last 201 images) in the level 0 folder\n",
    "i = 0\n",
    "labeled_folder_0_dir = \"data/labeled_data/0/\"\n",
    "labeled_folder_1_dir = \"data/labeled_data/1/\"\n",
    "labeled_folder_3_dir = \"data/labeled_data/3/\"\n",
    "labeled_folder_4_dir = \"data/labeled_data/4/\"\n",
    "\n",
    "# generate list of images 1 severity\n",
    "# generate list of images 2 severity\n",
    "# generate list of images 3 severity \n",
    "\n",
    "for image in labeled_folder_0_dir:\n",
    "    os.remove(image)\n",
    "    if i < 67: # replace 67 images from 0 severity to 1 severity\n",
    "        # look up image name in dictionary with value 1\n",
    "        # v is the severity grade (1)\n",
    "        temp_image = random.choice([img_1 for img_1,v in image_value_dic.items() if v == 1]) ##https://stackoverflow.com/questions/42438808/finding-all-the-keys-with-the-same-value-in-a-python-dictionary\n",
    "            \n",
    "            # if image name is not in severity folder 1 then\n",
    "        while(temp_image in labeled_folder_1_dir):\n",
    "            temp_image = random.choice([img_1 for img_1,v in image_value_dic.items() if v == 1])\n",
    "        # push into folder 1\n",
    "        shutil.copy(data_dir, labeled_folder_1_dir)\n",
    "    elif i < 134:\n",
    "        # look up image name in dictionary with value 3\n",
    "        # v is the severity grade (3)\n",
    "        temp_image = random.choice([img_3 for img_3,v in image_value_dic.items() if v == 3]) ##https://stackoverflow.com/questions/42438808/finding-all-the-keys-with-the-same-value-in-a-python-dictionary\n",
    "            \n",
    "            # if image name is not in severity folder 1 then\n",
    "        while(temp_image in labeled_folder_3_dir):\n",
    "            temp_image = random.choice([img_3 for img_3,v in image_value_dic.items() if v == 3])\n",
    "        # push into folder 1\n",
    "        shutil.copy(data_dir, labeled_folder_3_dir)\n",
    "    else:  # replace 67 images from 0 severity to 4 severity\n",
    "        # look up image name in dictionary with value 3\n",
    "        # v is the severity grade (4)\n",
    "        temp_image = random.choice([img_4 for img_4,v in image_value_dic.items() if v == 4]) ##https://stackoverflow.com/questions/42438808/finding-all-the-keys-with-the-same-value-in-a-python-dictionary\n",
    "            \n",
    "            # if image name is not in severity folder 1 then\n",
    "        while(temp_image in labeled_folder_4_dir):\n",
    "            temp_image = random.choice([img_4 for img_4,v in image_value_dic.items() if v == 4])\n",
    "        # push into folder 1\n",
    "        shutil.copy(data_dir, labeled_folder_4_dir)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc12746-c3d5-41ba-9cd5-2e891cf25bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "file_size_list = [num_of_0_imgs, num_of_1_imgs, \n",
    "                  num_of_2_imgs, num_of_3_imgs, \n",
    "                  num_of_4_imgs]  # adds up all of the images in the folders and sets it to a variable\n",
    "classes = [str(i) for i in range(5)]\n",
    "\n",
    "freq_c_vals = file_size_list   # freq(c)\n",
    "\n",
    "#store datasize while we are here\n",
    "datasize = sum(freq_c_vals)\n",
    "\n",
    "statsres = [statistics.median(freq_c_vals)/freq_c for freq_c in freq_c_vals]   # median_freq / freq(c)\n",
    "\n",
    "median_frequency_balancing = dict(zip(classes, statsres))\n",
    "median_frequency_balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03849448",
   "metadata": {},
   "source": [
    "# STEP 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf01bea",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "For preprocessing the images, the first step that is taken is to resize the images in the dataset into the same size which in the article was 600x600 pixels. \n",
    "\n",
    "The next step was to begin color normalization since the images were taken by different camera models which means that images can have distinct color temperatures and illumination so normalizing these colors can help reduce noise and variance and make the images more similar. \n",
    "\n",
    "In short this is done by getting the average value of red, blue, and green pixels in an image. Then median filtering is done which is used when trying to find and differentiate the features of an image such as the edges or discontinuities. \n",
    "\n",
    "The next step of pre-processing is using CLAHE, or Contrast Limited Adaptive Histogram Equalization, which helps deblur the image and remove noise from the image through local contrast enhancement which the images are now ready for optical disk segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d598a-0a89-4674-ac39-2f2eeda54c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocessing and augmentation\n",
    "# # resizing images to 600x600\n",
    "\n",
    "# img_count = 0\n",
    "# i = 0\n",
    "\n",
    "# # load image directory\n",
    "# img_dir = \"data/labeled_data\"\n",
    "\n",
    "# # iterate through labeled folders\n",
    "# for i in range(5):\n",
    "#     n_img_dir = img_dir + \"/\" + str(i)\n",
    "    \n",
    "#     # directory exists\n",
    "#     if (os.path.isdir(n_img_dir)):\n",
    "        \n",
    "#         # iterate through images\n",
    "#         for img in os.listdir(n_img_dir):\n",
    "            \n",
    "#             # check the file extension\n",
    "#             if (img.endswith(\".jpeg\")):\n",
    "                \n",
    "#                 # load image\n",
    "#                 r_img = Image.open(n_img_dir + \"\\\\\" + img)\n",
    "            \n",
    "#                 # image successfully loaded; perform transformations\n",
    "#                 r_img = r_img.resize((600, 600))\n",
    "                \n",
    "#                 #r_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9f526-b583-4a61-a752-fe3b632bb6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34966541/how-can-one-display-an-image-using-cv2-in-python\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "labeled_folder_0_dir = \"data/labeled_data/0/\"  # filepath for 0 severity image folder\n",
    "\n",
    "image = cv2.imread(labeled_folder_0_dir + \"10626_right.jpeg\")  # retrieves the spcific img called 1061_right.jpeg and sets it to image object chose this image because it has high amount of glare\n",
    "image = cv2.resize(image, (600,600))  # resizes image object to 600 x 600\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # cv2 reads in images as BGG so we convert the image to rgb\n",
    "(R,G,B) = (cv2.mean(image[0]), cv2.mean(image[1]), cv2.mean(image[2]))  # R, G, B = mean values of each channel\n",
    "\n",
    "# open the original image\n",
    "img = Image.open(\"data/labeled_data/0/10626_right.jpeg\")\n",
    "img.show()\n",
    "\n",
    "# iterate through all the pixels in the image\n",
    "for x in range(600):\n",
    "    for y in range(600):\n",
    "        \n",
    "        # find and assign the rgb values of the image\n",
    "        (r,g,b) = image[x,y]\n",
    "        \n",
    "        # if either r and the mean are 0s, the pixel value should still be 0\n",
    "        if not R[0]: \n",
    "            new_r = 0\n",
    "        else: \n",
    "        # otherwise, divide the channel value by the image's average and scale to colorspace\n",
    "            new_r = r/R[0] * 255\n",
    "        if not G[0]: \n",
    "            new_g = 0\n",
    "        else: \n",
    "            new_g = g/G[0] * 255\n",
    "        if not B[0]: \n",
    "            new_b = 0\n",
    "        else: \n",
    "            new_b = b/B[0] * 255\n",
    "        \n",
    "        image[x,y][0] = new_r\n",
    "        image[x,y][1] = new_g\n",
    "        image[x,y][2] = new_b\n",
    "\n",
    "plt.imshow(image)\n",
    "cv2.imwrite(\"10626_right.jpeg\", image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41339196",
   "metadata": {},
   "source": [
    "**Function for color normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34966541/how-can-one-display-an-image-using-cv2-in-python\n",
    "\n",
    "def color_normalize(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # cv2 reads in images as BGG so we convert the image to rgb\n",
    "    (R,G,B) = (cv2.mean(image[0]), cv2.mean(image[1]), cv2.mean(image[2]))  # R, G, B = mean values of each channel\n",
    "\n",
    "    # iterate through all the pixels in the image\n",
    "    for x in range(600):\n",
    "        for y in range(600):\n",
    "\n",
    "            # find and assign the rgb values of the image\n",
    "            (r,g,b) = image[x,y]\n",
    "\n",
    "            # if either r and the mean are 0s, the pixel value should still be 0\n",
    "            if not R[0]: \n",
    "                new_r = 0\n",
    "            else: \n",
    "            # otherwise, divide the channel value by the image's average and scale to colorspace\n",
    "                new_r = r/R[0] * 255\n",
    "            if not G[0]: \n",
    "                new_g = 0\n",
    "            else: \n",
    "                new_g = g/G[0] * 255\n",
    "            if not B[0]: \n",
    "                new_b = 0\n",
    "            else: \n",
    "                new_b = b/B[0] * 255\n",
    "\n",
    "            image[x,y][0] = new_r\n",
    "            image[x,y][1] = new_g\n",
    "            image[x,y][2] = new_b\n",
    "\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec6f7e4",
   "metadata": {},
   "source": [
    "**Example of using color normalization function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aebd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34966541/how-can-one-display-an-image-using-cv2-in-python\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "labeled_folder_0_dir = \"data/labeled_data/0/\"  # filepath for 0 severity image folder\n",
    "\n",
    "image = cv2.imread(labeled_folder_0_dir + \"10626_right.jpeg\")  # retrieves the spcific img called 1061_right.jpeg and sets it to image object chose this image because it has high amount of glare\n",
    "image = cv2.resize(image, (600,600))  # resizes image object to 600 x 600\n",
    "\n",
    "color_normalize(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c76a2a",
   "metadata": {},
   "source": [
    "## Median Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dddd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OpenCV and numpy\n",
    "# Need to install this in terminal: pip install opencv-python\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23612103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement median filtering\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test median filtering for one image to see if it works properly\n",
    "img = cv2.imread('data/labeled_data/0/10_left.jpeg')\n",
    "median = cv2.medianBlur(img, 5)\n",
    "\n",
    "# Show modified image\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9e47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement median filtering for all images in dataset\n",
    "# Use for loop from previous step which was resizing to 600x600\n",
    "\n",
    "# Load image directory\n",
    "img_dir = \"data/labeled_data\"\n",
    "\n",
    "# Iterate through labeled folders\n",
    "for i in range(5):\n",
    "    n_img_dir = img_dir + \"/\" + str(i)\n",
    "    print(n_img_dir)\n",
    "    \n",
    "    # Directory exists\n",
    "    if (os.path.isdir(n_img_dir)):\n",
    "        \n",
    "        # Iterate through images\n",
    "        for img in os.listdir(n_img_dir):\n",
    "            \n",
    "            # Check the file extension\n",
    "            if (img.endswith(\".jpeg\")):\n",
    "                print(img)\n",
    "                \n",
    "                # Load image            \n",
    "                img = cv2.imread(n_img_dir + \"/\" + img)\n",
    "                \n",
    "                # Median Filtering\n",
    "                median = cv2.medianBlur(img, 5)\n",
    "                \n",
    "                # Display original images\n",
    "                plt.title(\"Original\")\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "                \n",
    "                # Display images with median filter\n",
    "                plt.title(\"Median Filter\")\n",
    "                plt.imshow(median)\n",
    "                plt.show()\n",
    "                \n",
    "                # Need to save images with median filter applied\n",
    "                \n",
    "            break # Remove break to apply median filter to all images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08540b7-2060-40b2-83e4-1de53539e3e5",
   "metadata": {},
   "source": [
    "## CLAHE\n",
    "This takes images and increases the contrast of it to make the image more visually appealing and easier to see the edges.\n",
    "\n",
    "[Article explaining how to code with CLAHE](https://www.geeksforgeeks.org/clahe-histogram-eqalization-opencv/#:~:text=CLAHE%20is%20a%20variant%20of,to%20remove%20the%20artificial%20boundaries.)\n",
    "\n",
    "[Another article explaining CLAHE](https://towardsdatascience.com/clahe-and-thresholding-in-python-3bf690303e40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a0c2d-bc08-44be-86e6-1ebeb92eab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"data/labeled_data/0/10_left.jpeg\")  # gets the first image in the 0 folder and sets it to an image object\n",
    "image = cv2.resize(image, (600, 600))  # resizes the image to 600 x 600\n",
    "\n",
    "# This step from the tutorial should be redundent since we had already did median filtering and color normalization before\n",
    "image = cv2.medianBlur(image, 5) # Median filtering on image\n",
    "image_bw = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # convert image to grayscale\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit = 40) # creates CLAHE object; clip limit is threshold for contrast limiting and default is 40\n",
    "final_img = clahe.apply(image_bw) # applying CLAHE on image\n",
    " \n",
    "# Ordinary thresholding the same image\n",
    "_, ordinary_img = cv2.threshold(image_bw, 155, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Grayscale Image\")\n",
    "plt.imshow(image_bw)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Ordinary Threshold\")\n",
    "plt.imshow(ordinary_img)\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Final Image\")\n",
    "plt.imshow(final_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21582c28",
   "metadata": {},
   "source": [
    "**Create CLAHE function to apply CLAHE to an image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba6a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created CLAHE function that takes in a cv2 image object and returns an image object that can be shown with\n",
    "def clahe_function(image):\n",
    "    \n",
    "    # This step from the tutorial should be redundent since we had already did median filtering and color normalization before\n",
    "    # image = cv2.medianBlur(image, 5) # Median filtering on image\n",
    "    image_bw = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # convert image to grayscale\n",
    "    \n",
    "    clahe = cv2.createCLAHE(clipLimit = 40) # creates CLAHE object; clip limit is threshold for contrast limiting and default is 40\n",
    "    final_img = clahe.apply(image_bw) # applying CLAHE on image\n",
    " \n",
    "    # Ordinary thresholding the same image\n",
    "    _, ordinary_img = cv2.threshold(image_bw, 155, 255, cv2.THRESH_BINARY)\n",
    "    return final_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adc0a07",
   "metadata": {},
   "source": [
    "### Main Loop for Data Preprocessing\n",
    "\n",
    "**NOTE: Currently uses a test image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3088cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image directory\n",
    "img_dir = \"data/labeled_data\"\n",
    "\n",
    "# Iterate through labeled folders\n",
    "for i in range(5):\n",
    "    n_img_dir = img_dir + \"/\" + str(i)\n",
    "    print(n_img_dir)\n",
    "    \n",
    "    # Directory exists\n",
    "    if (os.path.isdir(n_img_dir)):\n",
    "        \n",
    "        # Iterate through images\n",
    "        for img in os.listdir(n_img_dir):\n",
    "            \n",
    "            # Check the file extension\n",
    "            if (img.endswith(\".jpeg\")):\n",
    "                print(img)\n",
    "                \n",
    "                # Load image            \n",
    "                # img = cv2.imread(n_img_dir + \"/\" + img)\n",
    "                \n",
    "                # test image\n",
    "                labeled_folder_0_dir = \"data/labeled_data/0/\"  # filepath for 0 severity image folder\n",
    "                img = cv2.imread(labeled_folder_0_dir + \"10626_right.jpeg\")\n",
    "                plt.title(\"Original Image\")\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "                \n",
    "                # Resize image object to 600 x 600\n",
    "                img = cv2.resize(img, (600,600))\n",
    "                plt.title(\"Resized Image\")\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "                \n",
    "                # color normalize\n",
    "                img = color_normalize(img)\n",
    "                plt.title(\"Image with Color Normalization\")\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "                \n",
    "                # Median Filtering\n",
    "                img = cv2.medianBlur(img, 5)\n",
    "                plt.title(\"Image with Median Filtering\")\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "                \n",
    "                # CLAHE\n",
    "                img = clahe_function(img) # applying CLAHE on image\n",
    "                plt.title(\"Image with CLAHE\")\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "                \n",
    "                # Need to save images with median filter applied\n",
    "                \n",
    "            break # Remove break to apply median filter to all images\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac91cb6",
   "metadata": {},
   "source": [
    "# STEP 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4011945",
   "metadata": {},
   "source": [
    "## Optical Disk Segmentation\n",
    "The third step in this process is Optical Disk (OD) Segmentation. In this step, the RGP channels are changed to HSV channels, and the OD is identified by applying the Morphological closing method and contour filling. \n",
    "\n",
    "The former dilates the area of interest and removes the borders, while the latter fills in the gaps. Finally, the OD-segmented image can be obtained by subtracting the results of the second part from the results of the first part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543991e5",
   "metadata": {},
   "source": [
    "# STEP 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54543ebc",
   "metadata": {},
   "source": [
    "## Split Images into 4 Quadrants\n",
    "The next step is to divide the images into four quadrants so that it is easier to view tiny-sized microaneurysms and other features that identify DR. \n",
    "\n",
    "First, each image was resized to 600x600 which is detailed in Step 2. However, the model is expecting images with an input size of 299x299, which is too small to discern between tiny features in the eye images and can result in the misidentification of microaneurysms. \n",
    "\n",
    "Therefore, these images are further cropped into four quadrants of size 300x300. Dividing the full image into four quadrants of 300x300 meet the input size criteria of the model while retaining the size of the original image to view tiny features more easily. Overall, cropping the eye images can help reduce misidentification of microaneurysms and signs of DR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://neuraspike.com/blog/split-and-manipulate-pixels-opencv/\n",
    "\n",
    "# Load the image, get it's special dimensions (width and height),\n",
    "# Then display the original image to our screen\n",
    "image = cv2.imread('data/labeled_data/0/10_left.jpeg')\n",
    "(h, w) = image.shape[:2]\n",
    "cv2.imshow('Original', image)\n",
    " \n",
    "# compute the center coordinate of the image\n",
    "(cX, cY) = (w // 2, h // 2)\n",
    "\n",
    "# crop the image into four parts which will be labelled as\n",
    "# top left, top right, bottom left, and bottom right.\n",
    "topLeft = image[0:cY, 0:cX]\n",
    "topRight = image[0:cY, cX:w]\n",
    "bottomLeft = image[cY:h, 0:cX]\n",
    "bottomRight = image[cY:h, cX:w]\n",
    " \n",
    "# visualize the cropped regions\n",
    "\n",
    "plt.imshow(topLeft)\n",
    "plt.show()\n",
    "plt.imshow(topRight)\n",
    "plt.show()\n",
    "plt.imshow(bottomLeft)\n",
    "plt.show()\n",
    "plt.imshow(bottomRight)\n",
    "plt.show()\n",
    "\n",
    "# cv2.imshow(\"Top Left Corner\", topLeft)\n",
    "# cv2.imshow(\"Top Right Corner\", topRight)\n",
    "# cv2.imshow(\"Bottom Right Corner\", bottomLeft)\n",
    "# cv2.imshow(\"Bottom Left Corner\", bottomRight)\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf02bada",
   "metadata": {},
   "source": [
    "# STEP 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef71e20",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "After the images are split into four quadrants, there are three main methods to augment the data. These steps include flipping (horizontal and vertical), 90â€“180Â° random rotation, and random zooming that ranges in between [0.85, 1.15]. The purpose of data augmentation in this step is to expand the training samples and strengthen the size of the class. Also, the combination of preprocessing and data augmentation makes the neural network insusceptible to variation attenuation, insufficient illumination, and changing orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8919fd1",
   "metadata": {},
   "source": [
    "### Flipping (horizontal and vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af463823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e307458",
   "metadata": {},
   "source": [
    "### 90-180 degrees random rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: http://crumbsofcode.com/rotating-images-python/\n",
    "import random\n",
    "\n",
    "img = Image.open('data/labeled_data/0/10_left.jpeg')\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "rotate = random.randint(0, 360)  # Generates a random degree from 0 to 360\n",
    "output = img.rotate(rotate, expand=True)  # Rotates image to the selected amount\n",
    "# output.save(\"output%s.jpg\" % count)  # Saves Every Rotation\n",
    "\n",
    "plt.imshow(output)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833756ca",
   "metadata": {},
   "source": [
    "### Random zooming that ranges between 0.85-1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec5065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "564d5760",
   "metadata": {},
   "source": [
    "# STEP 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4302377",
   "metadata": {},
   "source": [
    "## Data Feeding into InceptionResnet-V2 CNN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98396ab",
   "metadata": {},
   "source": [
    "### Load Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d027f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 files belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.utils\n",
    "# tf.keras.utils.image_dataset_from_directory(\n",
    "#     directory,\n",
    "#     labels=\"inferred\",\n",
    "#     label_mode=\"int\",\n",
    "#     class_names=None,\n",
    "#     color_mode=\"rgb\",\n",
    "#     batch_size=32,\n",
    "#     image_size=(256, 256),\n",
    "#     shuffle=True,\n",
    "#     seed=None,\n",
    "#     validation_split=None,\n",
    "#     subset=None,\n",
    "#     interpolation=\"bilinear\",\n",
    "#     follow_links=False,\n",
    "#     crop_to_aspect_ratio=False,\n",
    "#     **kwargs\n",
    "# )\n",
    "\n",
    "dataloader = tf.keras.utils.image_dataset_from_directory(data_dir, labels='inferred')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce4f65",
   "metadata": {},
   "source": [
    "### Instantiate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25736338",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpectedly found an instance of type `<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>`. Expected a symbolic tensor instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25340/4200257984.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m model = tf.keras.applications.InceptionResNetV2(\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0minclude_top\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"imagenet\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m#median frequency balancing?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\applications\\inception_resnet_v2.py\u001b[0m in \u001b[0;36mInceptionResNetV2\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[0mimg_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m       \u001b[0mimg_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1295\u001b[0m                      \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRaggedTensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m                      keras_tensor.KerasTensor)):\n\u001b[1;32m-> 1297\u001b[1;33m     raise ValueError('Unexpectedly found an instance of type `' + str(type(x)) +\n\u001b[0m\u001b[0;32m   1298\u001b[0m                      '`. Expected a symbolic tensor instance.')\n\u001b[0;32m   1299\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'tensorflow.python.data.ops.dataset_ops.BatchDataset'>`. Expected a symbolic tensor instance."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "\n",
    "# tf.keras.applications.InceptionResNetV2(\n",
    "#     include_top=True,\n",
    "#     weights=\"imagenet\",\n",
    "#     input_tensor=None,\n",
    "#     input_shape=None,\n",
    "#     pooling=None,\n",
    "#     classes=1000,\n",
    "#     classifier_activation=\"softmax\",\n",
    "#     **kwargs\n",
    "# )\n",
    "\n",
    "model = tf.keras.applications.InceptionResNetV2(\n",
    "    include_top = True,\n",
    "    weights = \"imagenet\", #median frequency balancing?\n",
    "    classes = 1000, #required 1000 for imagenet\n",
    "    classifier_activation = \"softmax\",\n",
    "    input_tensor = dataloader\n",
    ")\n",
    "\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d1bab",
   "metadata": {},
   "source": [
    "# STEP 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f13ff",
   "metadata": {},
   "source": [
    "## Depth Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49657f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44a17b4a",
   "metadata": {},
   "source": [
    "# STEP 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453e22a",
   "metadata": {},
   "source": [
    "## Network Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009530ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87a5032a",
   "metadata": {},
   "source": [
    "## Cross-entropy Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4199351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07250cb6",
   "metadata": {},
   "source": [
    "## Time Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f17afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc6410fb",
   "metadata": {},
   "source": [
    "# STEP 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc595fb",
   "metadata": {},
   "source": [
    "## MESSIDOR Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee099450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aef5c716",
   "metadata": {},
   "source": [
    "# STEP 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49cbdff",
   "metadata": {},
   "source": [
    "## Perform Validation using IDRiD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022384e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57539966",
   "metadata": {},
   "source": [
    "## Load data using ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d796a-28c5-4d1d-936b-f2eb34e5359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(root=\"data/labeled_data\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((600, 600))\n",
    "])\n",
    "\n",
    "transform"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
